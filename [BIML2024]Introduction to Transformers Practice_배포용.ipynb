{"cells":[{"cell_type":"markdown","metadata":{"id":"922I26qsVIn9"},"source":["# BIML 2024 Introduction to Transformer Practice\n","- 고려대학교 의과대학 전민지 교수, 봉현수, 임우택 조교\n","\n","### 실습 논문\n","- Huang, Xiao, et al. \"Moltrans: Molecular Interaction Transformer for drug-target interaction prediction.\" Bioinformatics (2021)\n","\n","### 실습 목표: Tansformer 모델 실습, Drug target interaction 예측 및 Interaction Map 시각화\n","1. Data load 및 전처리\n","2. Model 만들기\n","- Embedding Module\n","- Transformer Module\n","- Interaction Module\n","- Decoder Module\n","3. SMILES, Protein amino acid sequence의 substructure 정의\n","4. Loss function 및 Optimizer 정의\n","5. Training\n","6. Drug target interaction 예측\n","7. Interaction Map 시각화"]},{"cell_type":"markdown","metadata":{"id":"ouDQbz50VIn-"},"source":["## 실습을 위한 안내\n","\"<font color='45A07A'>## 코드 시작 ##</font>\" \"<font color='45A07A'>## 코드 종료 ##</font>\" 는 여러분이 직접 작성해야 하는 부분입니다.\n","\n","**먼저, 실습에 사용할 dataset을 다운로드해야합니다.**\n","\n","https://drive.google.com/file/d/1_TTqKyG9PwAAvqB4QwEeLRbXNbSRCdHb/view?usp=sharing\n","\n","위 링크에서 dataset을 다운받아 사용할 프로젝트 디렉토리에 옮겨 주세요.\n","\n","\n","## 0-1. Colab 사용자를 위한 안내\n","\n","**로컬(개인 노트북)이 아닌 Colab 환경에서 실행하고자 하시는 분을 위한 안내입니다.**\n","\n","**로컬에서 실행하실 분은 \"로컬 사용자를 위한 안내\"로 넘어가세요.**\n","\n","**먼저 GPU를 사용하기 위한 설정입니다.**\n","\n","`T4 RAM 디스크` 아이콘 옆의 `추가 연결 옵션` -> `런타임 유형 변경` -> 하드웨어 가속기를 `T4 GPU` 로 설정 후 저장\n","\n","**이제 구글 드라이브와 Colab을 연결합니다.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCptpTk5VIn_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707876186343,"user_tz":-540,"elapsed":35988,"user":{"displayName":"Hyunsu Bong","userId":"15753599564509898031"}},"outputId":"bdc6cb42-c59c-473a-f012-67ad7db9b199"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["## Google Drive mount\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)"]},{"cell_type":"markdown","source":["\n","`folder` 변수 명에 `본 주피터 노트북 파일(.ipynb)`이 위치한 경로를 작성해주세요!\n","\n","예를 들어, `본 주피터 노트북 파일(.ipynb)` 의 위치가 \"내 드라이브 > Colab Notebooks > biml2024\" 폴더 안에 있는 경우, \"Colab Notebooks/biml2024\" 를 작성하시면 됩니다.\n","\n","```python\n","project_dir = \"Colab Notebooks/biml2024\"\n","```"],"metadata":{"id":"Zs1UV5p0iW_z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xb3WrYzVIoA"},"outputs":[],"source":["import os\n","from pathlib import Path\n","## 코드 작성 ##\n","## !! /content/drive/MyDrive/ 까지는 지우고 작성하세요 !! ##\n","## !! 파일명도 지우고 작성하세요 !! ##\n","project_dir = None # 프로젝트를 저장한 디렉토리 예) \"Colab Notebooks/biml2024\"\n","\n","## 코드 종료\n","\n","base_path = Path(\"/content/drive/MyDrive/\")\n","project_path = base_path / project_dir\n","os.chdir(project_path)\n","for x in list(project_path.glob(\"*\")):\n","    if x.is_dir():\n","        dir_name = str(x.relative_to(project_path))\n","        os.rename(dir_name, dir_name.split(\" \", 1)[0])\n","print(f\"현재 디렉토리 위치: {os.getcwd()}\")"]},{"cell_type":"markdown","source":["\n","\n","```bash\n","CompletedProcess(args=['unzip', '-qq', '/content/drive/MyDrive/Colab Notebooks/biml2024/biml_files.zip', '-d', PosixPath('/content/drive/MyDrive/Colab Notebooks/biml2024')], returncode=0)\n","```\n","아래 코드를 실행하여\n","위와 같이 출력되고 압축된 파일들이 실제 드라이브 경로에 있는지 확인해주세요.\n","\n","**한 번만 실행하시면 됩니다.**\n"],"metadata":{"id":"nVvYJw7ZXC1u"}},{"cell_type":"code","source":["import subprocess\n","\n","target_file_path = f\"{project_path}/biml_files.zip\"\n","\n","command = [\"unzip\", \"-qq\", target_file_path, \"-d\", project_path]\n","subprocess.run(command)"],"metadata":{"id":"j77-P5zmXCNN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**프로젝트 디렉토리에 [.ipynb파일, dataset폴더, ESPF폴더, utils폴더]가 모두 있는지 확인해주세요.**"],"metadata":{"id":"Rd-LCaL6-jQi"}},{"cell_type":"markdown","source":["\n","## 0-2. 로컬 사용자를 위한 안내\n","\n","**Colab이 아닌 로컬(개인 노트북)에서 실행하고자 하시는 분을 위한 안내입니다.**"],"metadata":{"id":"qXrEcfOfIAn9"}},{"cell_type":"markdown","source":["**로컬 사용자**\n","\n","다운로드한 파일의 압축을 풀어주세요.\n","\n","**프로젝트 디렉토리에 [.ipynb파일, dataset폴더, ESPF폴더, utils폴더]가 모두 있는지 확인해주세요.**\n"],"metadata":{"id":"jhs2NTv9KdIF"}},{"cell_type":"markdown","metadata":{"id":"3F-cyDnqVIoA"},"source":["## 1. Package load\n","\n","**패키지를 설치해야합니다.**\n","\n","**Colab 사용자**\n","\n","아래 명령어를 실행하고 설치가 되는지 확인해주세요."]},{"cell_type":"code","source":["!pip install subword_nmt"],"metadata":{"id":"3_WCSaoSatGr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**로컬 사용자**\n","\n","아래 패키지가 설치되어 있는지 확인해보고 torch부터 설치가 되어있지 않다면 Colab으로 진행하시는 것을 권장합니다.\n"],"metadata":{"id":"FNGY0I41dQAd"}},{"cell_type":"markdown","source":["이제 패키지를 load하면 됩니다.\n","\n","- `torch` : pytorch를 load합니다.\n","- `torch.autograd - Variable` :\n","- `torch.utils` : 모델에 사용할 Dataset과 DataLoader를 구성하기 위해 사용합니다.\n","- `torch.nn` : 모델의 각 Layer들을 만들기 위해 사용합니다.\n","- `copy` :  deepcopy로 Encoder class를 layer 수만큼 복사하기 위해 사용합니다.\n","- `math` :  기본 수학 연산을 위해 사용합니다.\n","- `matplotlib.pyplot` :  데이터 시각화를 위해 사용합니다.\n","- `numpy` :  Scientific computing과 관련된 여러 편리한 기능들을 제공해주는 라이브러리입니다.\n","- `pandas` :  데이터를 load하기 위해 사용합니다.\n","- `subword_nmt.apply_bpe - BPE` : subword tokenizer 중 하나인 Byte Pair Encoding(BPE)를 불러옵니다.\n","- `codecs` : vocabulary를 load하기 위해 사용합니다.\n","- `sklearn` : 머신러닝 라이브러리로 모델 성능 평가를 위해 사용합니다."],"metadata":{"id":"6PKbG80MatgB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bF6YcsUyVIoA"},"outputs":[],"source":["import torch\n","from torch.autograd import Variable\n","from torch.utils import data\n","from torch import nn\n","import torch.nn.functional as F\n","\n","import copy\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from subword_nmt.apply_bpe import BPE\n","import codecs\n","\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, roc_curve, confusion_matrix, precision_score, recall_score, auc\n","\n","## Seed를 고정합니다.\n","torch.manual_seed(1)\n","np.random.seed(1)\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","\n","print('GPU 사용 가능 여부: {}'.format(use_cuda))"]},{"cell_type":"markdown","source":["**GPU 사용 가능 여부가 False로 뜨시는 분은 런타임을 확인해주세요.**\n","\n","`T4 RAM 디스크` 아이콘 옆의 `추가 연결 옵션` -> `런타임 유형 변경` -> 하드웨어 가속기를 `T4 GPU` 로 설정 후 저장"],"metadata":{"id":"ShZVNbgYG9Mt"}},{"cell_type":"markdown","metadata":{"id":"HFseZhzkVIoB"},"source":["## 2. 하이퍼파라미터 세팅\n","사용할 기본 하이퍼파라미터를 설정합니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hV47d_MkVIoB"},"outputs":[],"source":["config = {}\n","config['batch_size'] = 16 # 미니배치의 크기\n","config['input_dim_drug'] = 23532 # SMILES 길이\n","config['input_dim_target'] = 16693 # 단백질 아미노산 sequence 길이\n","config['train_epoch'] = 13 # 학습할 epoch 수\n","config['max_drug_seq'] = 50 # SMILES의 substructure 크기\n","config['max_protein_seq'] = 545 # 단백질 아미노산 sequence의 substructure 크기\n","config['emb_size'] = 384 # Embedding 크기\n","config['dropout_rate'] = 0.1 # Dropout rate\n","\n","# Encoder\n","config['intermediate_size'] = 1536 # Feed Forward dimension\n","config['num_attention_heads'] = 12 # Attention head 개수\n","config['attention_probs_dropout_prob'] = 0.1 # Dropout rate\n","config['hidden_dropout_prob'] = 0.1 # Dropout rate\n","config['flat_dim'] = 78192 # Decoder의 input 차원"]},{"cell_type":"markdown","metadata":{"id":"co0-WXquVIoB"},"source":["하이퍼파라미터는 뉴럴네트워크를 통하여 학습되는 것이 아니라 학습율(learning rate), 사용할 레이어의 수 등 설계자가 결정해줘야 하는 값들을 의미합니다.\n","\n","미니배치의 크기(`batch_size`), 학습 할 epoch 수(`train_epoch`), 학습률(`learning_rate`) 등의 값들을 다음과 같이 정하겠습니다.\n","\n","(1e-4는 0.0001과 같습니다.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LX_pDV8TVIoC"},"outputs":[],"source":["config['batch_size'] = 32\n","config['train_epoch'] = 1 # 시간 관계상 epoch는 1로 하겠습니다.\n","learning_rate = 5e-6"]},{"cell_type":"markdown","source":["## 3. SMILES, Protein amino acid sequence의 substructure 정의\n","\n","BPE(Byte Pair Encoding)는 문장 혹은 단어를 적절한 단위로 나누는 subword tokenizer 중 하나로, token들을 빈도를 기준으로 merge하여 새로운 token을 만듭니다.\n","\n","\n","\"RRQ S\", \"AKF S\", \"T AK\" 를 예시로 들면,\n","1. 먼저 각 token의 빈도를 계산합니다.\n","  (\"RRQ S\", 4), (\"AKF S\", 2), (\"T AK\" 6)\n","2. 띄어쓰기 단위로 나누어 각 단어의 빈도를 계산합니다.\n","  \n","  (\"RRQ\", 10), (\"S\", 5), (\"AKF\", 12), (\"T\", 4), (\"AK\", 6)\n","2. Character단위로 나누고, --> (\"R\" \"R\" \"Q\", 10), (\"S\", 5), (\"A\" \"K\" \"F\", 12), (\"T\", 4), (\"A\" \"K\", 6)\n","  \n","  각 char들을 연속하게 묶어 pair를 만들고 빈도 순으로 merge합니다.\n","  \n","  다음과 같은 pair가 만들어질 수 있고, --> \"RR\" \"RQ\" \"S\" \"AK\" \"KF\" \"T\"\n","  \n","  \"AK\" 의 빈도가 18로 가장 높으므로 이들을 merge합니다.\n","  \n","  (\"R\" \"R\" \"Q\", 10), (\"S\", 5), (\"AK\" \"F\", 12), (\"T\", 4), (\"AK\", 6)\n","  \n","  이 과정을 반복합니다."],"metadata":{"id":"QDXJZNyp_1c3"}},{"cell_type":"code","source":["vocab_path = './ESPF/protein_codes_uniprot.txt' # 아미노산 seq를 \"RRQ S\", \"AK S\", \"T AI\" 등으로 tokenize한 vocabulary\n","bpe_codes_protein = codecs.open(vocab_path) # Load vocabulary\n","pbpe = BPE(bpe_codes_protein, merges=-1, separator='') # BPE를 initialize합니다. merges=-1은 더 이상 merge가 불가능할 때까지 수행함을 의미합니다.\n","\n","sub_csv = pd.read_csv('./ESPF/subword_units_map_uniprot.csv') # 미리 정의된 substructure 정보\n","idx2word_p = sub_csv['index'].values\n","words2idx_p = dict(zip(idx2word_p, range(0, len(idx2word_p)))) # substructure에 대한 dictionary를 만듭니다.\n","\n","\n","vocab_path = './ESPF/drug_codes_chembl.txt' # SMILES를 \"cc c\", \"( C\", \"C (=O)\" 등으로 tokenize한 vocabulary\n","bpe_codes_drug = codecs.open(vocab_path) # Load vocabulary\n","dbpe = BPE(bpe_codes_drug, merges=-1, separator='') # BPE를 initialize합니다.\n","\n","sub_csv = pd.read_csv('./ESPF/subword_units_map_chembl.csv') # 미리 정의된 substructure 정보\n","idx2word_d = sub_csv['index'].values\n","words2idx_d = dict(zip(idx2word_d, range(0, len(idx2word_d)))) # substructure에 대한 dictionary를 만듭니다."],"metadata":{"id":"CbV6XXVG_1Je"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def drug2emb_encoder(x):\n","    # x : Cc1ccc(CNS(=O)(=O)c2ccc(s2)S(N)(=O)=O)cc1\n","    max_d = 50 # Substructure의 최대 크기 (Drug)\n","    ## BPE로 token들을 빈도를 기준으로 merge해가며 새로운 token(substructure)을 만듭니다.\n","    t1 = dbpe.process_line(x).split()  # ['Cc1ccc(CN', 'S(=O)(=O)', 'c2ccc(s', '2)S', '(N)', '(=O)=O)', 'cc1']\n","    ## dictionary에서 substructure의 index를 가져옵니다.\n","    try:\n","        i1 = np.asarray([words2idx_d[i] for i in t1])  # [ 1842   121  2973 10592  1465  1061   106]\n","    except:\n","        i1 = np.array([0])\n","\n","    l = len(i1)\n","\n","    if l < max_d:\n","        i = np.pad(i1, (0, max_d - l), 'constant', constant_values = 0) # max_d보다 substructure의 크기가 작은 경우 차원을 확장합니다.\n","        input_mask = ([1] * l) + ([0] * (max_d - l)) # max_d보다 substructure의 크기가 작은 경우 나머지를 0으로 채웁니다.\n","\n","    else:\n","        i = i1[:max_d]\n","        input_mask = [1] * max_d\n","\n","    ## 구성한 substructure와 DTI array를 반환합니다.\n","    return i, np.asarray(input_mask)\n","\n","def protein2emb_encoder(x):\n","    max_p = 545  # Substructure의 최대 크기 (Protein)\n","    t1 = pbpe.process_line(x).split()\n","    try:\n","        i1 = np.asarray([words2idx_p[i] for i in t1])\n","    except:\n","        i1 = np.array([0])\n","\n","    l = len(i1)\n","\n","    if l < max_p:\n","        i = np.pad(i1, (0, max_p - l), 'constant', constant_values = 0)\n","        input_mask = ([1] * l) + ([0] * (max_p - l))\n","    else:\n","        i = i1[:max_p]\n","        input_mask = [1] * max_p\n","\n","    return i, np.asarray(input_mask)"],"metadata":{"id":"fCG5Yes2_7J3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DQRVXNLzVIoC"},"source":["## 4. `Custom Dataset` 및 `torch.utils.data.DataLoader` 구성\n","\n","본 실습에서 사용할 데이터는 `BindingDB` 입니다.\n","\n","`BindingDB`로 학습해보겠습니다."]},{"cell_type":"code","source":["## Custom Dataset\n","class BIN_Data_Encoder(data.Dataset):\n","    ## Dataset 구성에 필요한 변수 선언\n","    def __init__(self, list_IDs, labels, df_dti):\n","        'Initialization'\n","        self.labels = labels # DTI 여부 (1: True, 0: False)\n","        self.list_IDs = list_IDs # Sample Index\n","        self.df = df_dti # DataFrame\n","\n","    def __len__(self):\n","        ## Sample의 총 개수를 반환합니다.\n","        return len(self.list_IDs)\n","\n","    def __getitem__(self, index):\n","        ## 해당 Index의 SMILES와 단백질 아미노산 Sequence에 대한 Substructure를 구성합니다.\n","        index = self.list_IDs[index] # 해당 Index\n","        # d = self.df.iloc[index]['DrugBank ID']\n","        d = self.df.iloc[index]['SMILES'] # 해당 Index의 SMILES\n","        p = self.df.iloc[index]['Target Sequence'] # 해당 Index의 단백질 아미노산 Sequence\n","\n","        ## 구성한 substructure와 DTI array\n","        d_v, input_mask_d = drug2emb_encoder(d)\n","        p_v, input_mask_p = protein2emb_encoder(p)\n","\n","        y = self.labels[index] # 해당 Index의 DTI 여부 (1: True, 0: False)\n","        return d_v, p_v, input_mask_d, input_mask_p, y"],"metadata":{"id":"RQD8BBhQCznn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mb9lY1MWVIoC"},"outputs":[],"source":["params = {'batch_size': config['batch_size'], # 배치 크기\n","          'shuffle': True, # Data를 섞어서 사용\n","          'num_workers': 6, # Data를 불러올 때 6개의 process를 사용\n","          'drop_last': True # 마지막 배치를 사용하지 않으지 여부\n","          }\n","\n","## Train, Validation, Test 데이터를 load합니다.\n","dataFolder = './dataset/BindingDB'\n","df_train = pd.read_csv(dataFolder + '/train.csv')\n","df_val = pd.read_csv(dataFolder + '/val.csv')\n","df_test = pd.read_csv(dataFolder + '/test.csv')"]},{"cell_type":"code","source":["## Custom Dataset를 구성하고 미니 배치단위로 불러올 수 있도록 DataLoader에 넣어줍니다.\n","training_set = BIN_Data_Encoder(df_train.index.values, df_train.Label.values, df_train)\n","training_generator = data.DataLoader(training_set, **params)\n","\n","validation_set = BIN_Data_Encoder(df_val.index.values, df_val.Label.values, df_val)\n","validation_generator = data.DataLoader(validation_set, **params)\n","\n","testing_set = BIN_Data_Encoder(df_test.index.values, df_test.Label.values, df_test)\n","testing_generator = data.DataLoader(testing_set, **params)"],"metadata":{"id":"McGyf-KrHZhy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ioLwpdy8VIoC"},"source":["## 4. Model 만들기\n","\n","아래는 우리가 사용할 MolTrans 모델의 전체 구조입니다.\n","\n","첫번째 Cell에 있는 \"<font color='45A07A'>## 코드 시작 ##</font>\" 부분은 잠깐 넘어가고 나중에 다시 돌아와서 추가하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRDmSaOjVIoC"},"outputs":[],"source":["class BIN_Interaction_Flat(nn.Sequential):\n","\n","    def __init__(self, **config):\n","        super(BIN_Interaction_Flat, self).__init__()\n","        self.max_d = config['max_drug_seq']\n","        self.max_p = config['max_protein_seq']\n","        self.emb_size = config['emb_size']\n","        self.dropout_rate = config['dropout_rate']\n","\n","        ## densenet\n","        self.batch_size = config['batch_size']\n","        self.input_dim_drug = config['input_dim_drug']\n","        self.input_dim_target = config['input_dim_target']\n","        self.gpus = torch.cuda.device_count()\n","        self.n_layer = 2\n","        ## encoder\n","        self.hidden_size = config['emb_size']\n","        self.intermediate_size = config['intermediate_size']\n","        self.num_attention_heads = config['num_attention_heads']\n","        self.attention_probs_dropout_prob = config['attention_probs_dropout_prob']\n","        self.hidden_dropout_prob = config['hidden_dropout_prob']\n","\n","        self.flatten_dim = config['flat_dim']\n","\n","        ## Embedding\n","        ## Positional Embedding을 포함합니다.\n","        self.demb = Embeddings(self.input_dim_drug, self.emb_size, self.max_d, self.dropout_rate)\n","        self.pemb = Embeddings(self.input_dim_target, self.emb_size, self.max_p, self.dropout_rate)\n","\n","        ## Multi-head Attention을 위한 layer입니다.\n","        ## Feed Forward Network와 Residual Connection, Layer Normalization을 포함합니다.\n","        self.d_encoder = Encoder_MultipleLayers(self.n_layer, self.hidden_size, self.intermediate_size, self.num_attention_heads, self.attention_probs_dropout_prob, self.hidden_dropout_prob)\n","        self.p_encoder = Encoder_MultipleLayers(self.n_layer, self.hidden_size, self.intermediate_size, self.num_attention_heads, self.attention_probs_dropout_prob, self.hidden_dropout_prob)\n","\n","        ## Interaction Module의 neighborhood interaction을 위한 Convolution Layer입니다.\n","        self.icnn = nn.Conv2d(1, 3, 3, padding = 0)\n","\n","        ## Decoder입니다.\n","        self.decoder = nn.Sequential(\n","            nn.Linear(self.flatten_dim, 512),\n","            nn.ReLU(True),\n","\n","            nn.BatchNorm1d(512),\n","            nn.Linear(512, 64),\n","            nn.ReLU(True),\n","\n","            nn.BatchNorm1d(64),\n","            nn.Linear(64, 32),\n","            nn.ReLU(True),\n","\n","            ## output layer\n","            nn.Linear(32, 1)\n","        )\n","\n","    def forward(self, d, p, d_mask, p_mask):\n","\n","        ex_d_mask = d_mask.unsqueeze(1).unsqueeze(2)\n","        ex_p_mask = p_mask.unsqueeze(1).unsqueeze(2)\n","\n","        ex_d_mask = (1.0 - ex_d_mask) * -10000.0\n","        ex_p_mask = (1.0 - ex_p_mask) * -10000.0\n","\n","        d_emb = self.demb(d) # batch_size x seq_length x embed_size\n","        p_emb = self.pemb(p)\n","\n","        # 모든 encoder의 output을 가져오지 않습니다. 마지막 encoder block의 output만을 사용합니다.\n","        d_encoded_layers = self.d_encoder(d_emb.float(), ex_d_mask.float())\n","        p_encoded_layers = self.p_encoder(p_emb.float(), ex_p_mask.float())\n","\n","        # 두 tensor의 dimension을 맞춰줍니다. Dot product 및 Convolution operation을 진행하기 위함입니다.\n","        d_aug = torch.unsqueeze(d_encoded_layers, 2).repeat(1, 1, self.max_p, 1) # repeat along protein size\n","        p_aug = torch.unsqueeze(p_encoded_layers, 1).repeat(1, self.max_d, 1, 1) # repeat along drug size\n","\n","        i = d_aug * p_aug # 그림1. matrix I = dot product(E^p, E^d) -> Interaction Visualization에 사용합니다.\n","        i_v = i.view(int(self.batch_size/self.gpus), -1, self.max_d, self.max_p)\n","        i_v = torch.sum(i_v, dim = 1)\n","        i_v = torch.unsqueeze(i_v, 1)\n","\n","        i_v = F.dropout(i_v, p = self.dropout_rate)\n","\n","        f = self.icnn(i_v) # 그림2. matrix O = Convolution Neural Network(I)\n","\n","        f = f.view(int(self.batch_size/self.gpus), -1)\n","\n","        ## Decoder를 거친 후 DTI prediction합니다.\n","        score = self.decoder(f)\n","        return score\n","\n","    ## 코드 시작 (지금은 일단 넘어가세요) ##\n","    def get_interaction(self, d, p, d_mask, p_mask):\n","\n","        ex_d_mask = d_mask.unsqueeze(1).unsqueeze(2)\n","        ex_p_mask = p_mask.unsqueeze(1).unsqueeze(2)\n","\n","        ex_d_mask = (1.0 - ex_d_mask) * -10000.0\n","        ex_p_mask = (1.0 - ex_p_mask) * -10000.0\n","\n","        d_emb = self.demb(d) # batch_size x seq_length x embed_size\n","        p_emb = self.pemb(p)\n","\n","        # 모든 encoder의 output을 가져오지 않습니다. 마지막 encoder block의 output만을 사용합니다.\n","        d_encoded_layers = self.d_encoder(d_emb.float(), ex_d_mask.float())\n","        p_encoded_layers = self.p_encoder(p_emb.float(), ex_p_mask.float())\n","\n","        # 두 tensor의 dimension을 맞춰줍니다. Dot product 및 Convolution operation을 진행하기 위함입니다.\n","        d_aug = torch.unsqueeze(d_encoded_layers, 2).repeat(1, 1, self.max_p, 1) # repeat along protein size\n","        p_aug = torch.unsqueeze(p_encoded_layers, 1).repeat(1, self.max_d, 1, 1) # repeat along drug size\n","\n","        i = d_aug * p_aug # 그림1. matrix I = dot product(E^p, E^d) -> Interaction Visualization에 사용합니다.\n","        i_v = i.view(int(self.batch_size/self.gpus), -1, self.max_d, self.max_p)\n","        i_v = torch.sum(i_v, dim = 1)\n","\n","        return i, i_v\n","    ## 코드 종료 ##"]},{"cell_type":"markdown","source":["### 그림1.\n","<img src=\"https://drive.google.com/uc?export=download&id=1cCqFfcQuGkBvjbU0bVsJKO8mtwo_67W9\" width=\"900px\" height=\"500px\" />\n","\n","### 그림2.\n","<img src=\"https://drive.google.com/uc?export=download&id=1l6Cg_2Mnc1BUrh3lrSU5gys2F_e73dkS\" width=\"900px\" height=\"500px\" />"],"metadata":{"id":"Mw-NavIrvhTH"}},{"cell_type":"markdown","metadata":{"id":"P1usbjUiVIoD"},"source":["## 4-1. Embedding 및 Transformer Layer\n","\n","<img src=\"https://drive.google.com/uc?export=download&id=1s5aDmbls67YWL0KuRM75hubvIYLqmYkU\" width=\"900px\" height=\"500px\" />"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luCfglXgVIoD"},"outputs":[],"source":["## Layer Normalization Module입니다.\n","class LayerNorm(nn.Module):\n","    def __init__(self, hidden_size, variance_epsilon=1e-12):\n","\n","        super(LayerNorm, self).__init__()\n","        self.gamma = nn.Parameter(torch.ones(hidden_size))\n","        self.beta = nn.Parameter(torch.zeros(hidden_size))\n","        self.variance_epsilon = variance_epsilon\n","\n","    def forward(self, x):\n","        u = x.mean(-1, keepdim=True) # Layer normalization이므로 input을 기준으로 계산합니다.\n","        s = (x - u).pow(2).mean(-1, keepdim=True)\n","        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n","        return self.gamma * x + self.beta\n","\n","## Substructure를 Embedding합니다.\n","class Embeddings(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, max_position_size, dropout_rate):\n","        super(Embeddings, self).__init__()\n","        ## 코드 시작 ##\n","\n","        ## vocab_size(input_dim_drug or target)를 hidden_size(emb_size)차원의 vector로 변환합니다.\n","        self.word_embeddings = nn.Embedding(num_embeddings=None, embedding_dim=None)\n","\n","        ## max_position_size(max_d or p)를 hidden_size(emb_size)차원의 vector로 변환합니다.\n","        ## -> Positional Embedding을 위한 과정입니다.\n","        self.position_embeddings = None\n","\n","        ## 코드 종료 ##\n","\n","        self.LayerNorm = LayerNorm(hidden_size) # Layer Normalization\n","        self.dropout = nn.Dropout(dropout_rate) # Dropout\n","\n","    def forward(self, input_ids):\n","        seq_length = input_ids.size(1)\n","        ## 0~seq_length 만큼의 tensor array를 만들고 차원을 input_ids와 맞춥니다.\n","        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n","        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids) # Word embedding\n","        position_embeddings = self.position_embeddings(position_ids) # Positional Embedding\n","\n","        ## 코드 시작 ##\n","\n","        # 최종 Embedding\n","        embeddings = None # Positional Embedding 반영 (슬라이드 p.11 참고)\n","        embeddings = None # Layer Normalization\n","        embeddings = None # Dropout\n","\n","        ## 코드 시작 ##\n","\n","        return embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QX52k_OkVIoD"},"outputs":[],"source":["## 슬라이드 p.36 참고\n","## Multi-head Attention을 위한 layer입니다.\n","class SelfAttention(nn.Module):\n","    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob):\n","        super(SelfAttention, self).__init__()\n","        if hidden_size % num_attention_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (hidden_size, num_attention_heads))\n","\n","        self.num_attention_heads = num_attention_heads # Attention head 정의\n","        self.attention_head_size = int(hidden_size / num_attention_heads) # Memory 사용량을 줄이기 위해 head수로 query, key, value를 나눕니다.\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        ## 코드 시작 ##\n","        self.query = nn.Linear(hidden_size, self.all_head_size) # Query linear projection\n","        self.key = None # Key linear projection\n","        self.value = None # Value linear projection\n","        ## 코드 종료 ##\n","\n","        self.dropout = nn.Dropout(attention_probs_dropout_prob)\n","\n","    def transpose_for_scores(self, x):\n","        ## head수로 query, key, value를 나눕니다.\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3) # (batch_size, num_heads, q|k|v length, d_model/num_heads)\n","\n","    def forward(self, hidden_states, attention_mask):\n","        ## 코드 시작 ##\n","        mixed_query_layer = None # Query를 만듭니다. (W^Q projection) - (batch_size, query length, d_model)\n","        mixed_key_layer = None # Key를 만듭니다. (W^K projection) - (batch_size, key length, d_model)\n","        mixed_value_layer = None # Value를 만듭니다. (W^V projection) - (batch_size, value length, d_model)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer) # Head별로 query를 나눕니다. (batch_size, num_heads, query length, d_model/num_heads)\n","        key_layer = self.transpose_for_scores(mixed_key_layer) # Head별로 key를 나눕니다. (batch_size, num_heads, key length, d_model/num_heads)\n","        value_layer = self.transpose_for_scores(mixed_value_layer) # Head별로 value를 나눕니다. (batch_size, num_heads, value length, d_model/num_heads)\n","\n","        attention_scores = None # Query, Key.tanspose를 dot product(torch.matmul 사용)하여 Attention Score(Attention Weight)를 계산합니다. (차원을 유지하기 위해 (-1,-2)를 적용)\n","        attention_scores = None # Key layer의 dimension( size()[-1] )으로 divide합니다. (square root는 math.sqrt 사용)\n","\n","        attention_scores = attention_scores + attention_mask # masking합니다.\n","        attention_probs = nn.Softmax(dim=-1)(None) # Attention Distribution을 만듭니다.\n","\n","        attention_probs = self.dropout(attention_probs)\n","\n","        context_layer = None # Context Value(Attention Value)를 계산합니다. (torch.matmul 사용)\n","        ## 코드 종료 ##\n","\n","        ## Head Concatenate\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","        return context_layer\n","\n","## Feed Forward(W^O layer), Residual Connection, Layer Normalization\n","class SelfOutput(nn.Module):\n","    def __init__(self, hidden_size, hidden_dropout_prob):\n","        super(SelfOutput, self).__init__()\n","        self.dense = nn.Linear(hidden_size, hidden_size)\n","        self.dropout = nn.Dropout(hidden_dropout_prob)\n","        self.LayerNorm = LayerNorm(hidden_size)\n","\n","    def forward(self, hidden_states, input_tensor):\n","        hidden_states = self.dense(hidden_states) # Feed Forward(W^O layer)\n","        hidden_states = self.dropout(hidden_states)\n","\n","        ## Residual Connection 및 Layer Normalization입니다. (Attention output의 Feed Forward output + Input)\n","        ## 코드 시작 ##\n","        hidden_states = None\n","        ## 코드 종료 ##\n","\n","        return hidden_states\n","\n","# Attention module입니다.\n","class Attention(nn.Module):\n","    def __init__(self, hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n","        super(Attention, self).__init__()\n","        self.self = SelfAttention(hidden_size, num_attention_heads, attention_probs_dropout_prob) # Self-Attention\n","        self.output = SelfOutput(hidden_size, hidden_dropout_prob) # Feed Forward(W^O layer), Residual Connection, Layer Normalization\n","\n","    def forward(self, input_tensor, attention_mask):\n","        self_output = self.self(input_tensor, attention_mask)\n","        attention_output = self.output(self_output, input_tensor)\n","        return attention_output"]},{"cell_type":"markdown","metadata":{"id":"HOW-17TwVIoD"},"source":["<img src=\"https://drive.google.com/uc?export=download&id=1hDf1TvU0D0xq1fQiu21J9ZCOZxT-WB7g\" width=\"900px\" height=\"500px\" />"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0OTvV5rVIoD"},"outputs":[],"source":["# Feed Forward Network 입니다.\n","class Intermediate(nn.Module):\n","    def __init__(self, hidden_size, intermediate_size):\n","        super(Intermediate, self).__init__()\n","        self.dense = nn.Linear(hidden_size, intermediate_size)\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = F.relu(hidden_states)\n","        return hidden_states\n","\n","## Feed Forward Network, Residual Connection, Layer Normalization\n","class Output(nn.Module):\n","    def __init__(self, intermediate_size, hidden_size, hidden_dropout_prob):\n","        super(Output, self).__init__()\n","        self.dense = nn.Linear(intermediate_size, hidden_size)\n","        self.LayerNorm = LayerNorm(hidden_size)\n","        self.dropout = nn.Dropout(hidden_dropout_prob)\n","\n","    def forward(self, hidden_states, input_tensor):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","\n","        ## Residual Connection 및 Layer Normalization입니다. (Add & Normalize output + Input(Attention output))\n","        ## 코드 시작 ##\n","        hidden_states = None\n","        ## 코드 종료 ##\n","\n","        return hidden_states\n","\n","## Encoder block입니다.\n","class Encoder(nn.Module):\n","    def __init__(self, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n","        super(Encoder, self).__init__()\n","        self.attention = Attention(hidden_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob) # Self-Attention\n","        self.intermediate = Intermediate(hidden_size, intermediate_size) # Feed Forward Network\n","        self.output = Output(intermediate_size, hidden_size, hidden_dropout_prob) # Feed Forward Network, Residual Connection, Layer Normalization\n","\n","    def forward(self, hidden_states, attention_mask):\n","\n","        ## 앞에서 작성했던 코드를 참고하여 작성해주세요. ##\n","        ## 코드 시작 ##\n","        attention_output = None # Self-Attention (forward에 필요한 parameter를 참고하세요)\n","        intermediate_output = None # Attention output -> FFNN\n","        layer_output = None # FFNN, Residual Connection, Layer Normalization (forward에 필요한 parameter를 참고하세요)\n","        ## 코드 종료 ##\n","\n","        return layer_output\n","\n","## 슬라이드 p.37 참고\n","## Encoder block을 여러개 쌓습니다.\n","class Encoder_MultipleLayers(nn.Module):\n","    def __init__(self, n_layer, hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob):\n","        super(Encoder_MultipleLayers, self).__init__()\n","\n","        layer = Encoder(hidden_size, intermediate_size, num_attention_heads, attention_probs_dropout_prob, hidden_dropout_prob)\n","        ## Encoder block을 n_layer개 만큼 쌓습니다.\n","        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layer)])\n","\n","    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n","        all_encoder_layers = []\n","\n","        ## n_layer개 만큼 쌓은 encoder block을 for loop로 call합니다.\n","        ## 각 Encoder block의 output은 다음 Encoder block의 input으로 들어갑니다.\n","        for layer_module in self.layer:\n","            hidden_states = layer_module(hidden_states, attention_mask)\n","\n","        return hidden_states"]},{"cell_type":"markdown","metadata":{"id":"PqiW86meVIoE"},"source":["## 4-4. Model load\n","이제 우리가 정의한 모델을 불러오겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbE_IpXnVIoE"},"outputs":[],"source":["model = BIN_Interaction_Flat(**config)\n","model = model.cuda()"]},{"cell_type":"markdown","metadata":{"id":"oiG9vHkqVIoE"},"source":["## 5. Loss function 및 Optimizer 정의\n","\n","생성한 모델을 학습 시키기 위해서 손실함수를 정의해야 합니다.\n","\n","Neural Network는 경사하강(gradient descent)방법을 이용하여 손실함수의 값을 줄이는 방향으로 파라미터를 갱신(update) 하게 됩니다.\n","\n","우리는 Adam optimizer를 사용하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8bFcYCDVIoE"},"outputs":[],"source":["criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"v7exrZFsVIoE"},"source":["## 6. Training\n","\n","이제 모델에 데이터를 미니배치 단위로 제공해서 학습을 시킬 단계입니다.\n","\n","학습은 epoch 당 interation을 거치며 총 epoch * iteration 만큼 진행됩니다."]},{"cell_type":"markdown","metadata":{"id":"X2rgdJ-DVIoE"},"source":["Validation/Test 시에 사용할 기반 코드를 작성하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hD-27UL7VIoE"},"outputs":[],"source":["def test(data_generator, model):\n","    y_pred = []\n","    y_label = []\n","    model.eval()\n","    loss_accumulate = 0.0\n","    count = 0.0\n","    for i, (d, p, d_mask, p_mask, label) in enumerate(data_generator):\n","        score = model(d.long().cuda(), p.long().cuda(), d_mask.long().cuda(), p_mask.long().cuda())\n","\n","        m = torch.nn.Sigmoid()\n","        logits = torch.squeeze(m(score))\n","        loss_fct = torch.nn.BCELoss()\n","\n","        label = Variable(torch.from_numpy(np.array(label)).float()).cuda()\n","\n","        loss = loss_fct(logits, label)\n","\n","        loss_accumulate += loss\n","        count += 1\n","\n","        logits = logits.detach().cpu().numpy()\n","\n","        label_ids = label.to('cpu').numpy()\n","        y_label = y_label + label_ids.flatten().tolist()\n","        y_pred = y_pred + logits.flatten().tolist()\n","\n","    loss = loss_accumulate / count\n","\n","    fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n","\n","    precision = tpr / (tpr + fpr)\n","\n","    f1 = 2 * precision * tpr / (tpr + precision + 0.00001)\n","\n","    thred_optim = thresholds[5:][np.argmax(f1[5:])]\n","\n","    print(\"optimal threshold: \" + str(thred_optim))\n","\n","    y_pred_s = [1 if i else 0 for i in (y_pred >= thred_optim)]\n","\n","    auc_k = auc(fpr, tpr)\n","    print(\"AUROC:\" + str(auc_k))\n","    print(\"AUPRC: \" + str(average_precision_score(y_label, y_pred)))\n","\n","    cm1 = confusion_matrix(y_label, y_pred_s)\n","    print('Confusion Matrix : \\n', cm1)\n","    print('Recall : ', recall_score(y_label, y_pred_s))\n","    print('Precision : ', precision_score(y_label, y_pred_s))\n","\n","    total1 = sum(sum(cm1))\n","    #####from confusion matrix calculate accuracy\n","    accuracy1 = (cm1[0, 0] + cm1[1, 1]) / total1\n","    print('Accuracy : ', accuracy1)\n","\n","    sensitivity1 = cm1[0, 0] / (cm1[0, 0] + cm1[0, 1])\n","    print('Sensitivity : ', sensitivity1)\n","\n","    specificity1 = cm1[1, 1] / (cm1[1, 0] + cm1[1, 1])\n","    print('Specificity : ', specificity1)\n","\n","    outputs = np.asarray([1 if i else 0 for i in (np.asarray(y_pred) >= 0.5)])\n","    return roc_auc_score(y_label, y_pred), average_precision_score(y_label, y_pred), f1_score(y_label, outputs), y_pred, loss.item()"]},{"cell_type":"markdown","metadata":{"id":"HlKhCbrWVIoE"},"source":["학습이 잘 진행되고 있는지는 어떻게 알 수 있을까요?\n","\n","손실함수의 값 loss를 epoch와 iteration마다 출력하면 됩니다.\n","\n","우리는 다음과 같은 형식의 출력이 나오도록 만들겠습니다.\n","\n","출력 횟수 또한 매 iter마다 출력되는 것은 너무 많으므로 100번에 한 번씩 출력되도록 하겠습니다.\n","\n","**Training at Epoch 1 iteration 0 with loss 0.6687578**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOSUrZg8VIoE"},"outputs":[],"source":["def main():\n","    max_auc = 0\n","    loss_history = []\n","    loss_history_val = []\n","\n","    for epoch in range(config['train_epoch']):\n","        model.train()\n","        for i, (d, p, d_mask, p_mask, label) in enumerate(training_generator):\n","            output = model(d.long().cuda(), p.long().cuda(), d_mask.long().cuda(), p_mask.long().cuda())\n","\n","            label = Variable(torch.from_numpy(np.array(label)).float()).cuda()\n","            m = torch.nn.Sigmoid()\n","            pred = torch.squeeze(m(output))\n","\n","            loss = criterion(pred, label)\n","            loss_history.append(loss)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            if (i % 100 == 0):\n","                print('Training at Epoch ' + str(epoch + 1) + ' iteration ' + str(i) + ' with loss ' + str(loss.cpu().detach().numpy()))\n","\n","        with torch.no_grad():\n","            auc, auprc, f1, _, loss = test(validation_generator, model)\n","            loss_history_val.append(loss)\n","            print('Validation at Epoch '+ str(epoch + 1) + ' , AUROC: '+ str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1))\n","\n","    with torch.no_grad():\n","        auc, auprc, f1, _, loss = test(testing_generator, model)\n","\n","        print('Testing AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1) + ' , Test loss: '+str(loss))"]},{"cell_type":"markdown","metadata":{"id":"2ZJsIxFhVIoF"},"source":["**이제 학습을 시작하겠습니다. 아래 함수를 실행하세요.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WwYdULgpVIoF"},"outputs":[],"source":["main()"]},{"cell_type":"markdown","metadata":{"id":"rLt2sXHtVIoF"},"source":["## 7. Interaction Map\n","\n","Drug, Protein Substructure간 interaction을 확인해보겠습니다.\n","\n","먼저, Interaction을 확인하기 위한 함수를 만들어야합니다.\n","\n","**4. Model 만들기** 로 이동해서 함수를 추가한 후 돌아와서 진행하겠습니다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ieHY7X--VIoF"},"outputs":[],"source":["def show_interaction(data_generator):\n","    model.eval()\n","    for i, (d, p, d_mask, p_mask, label) in enumerate(data_generator):\n","        i, i_v = model.get_interaction(d.long().cuda(), p.long().cuda(), d_mask.long().cuda(), p_mask.long().cuda())\n","        return d, p, i, i_v\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahV-yUHUVIoF"},"outputs":[],"source":["d, p, i, i_v = show_interaction(testing_generator)\n","drug, protein = d[2], p[2]\n","drug, protein = drug[drug != 0], protein[protein != 0]\n","drug, protein"]},{"cell_type":"markdown","source":["아래는 substructure를 만들기 위한 vocabulary입니다.\n","\n","이를 활용해 index로 표기되어 있는 substructure를 SMILES, amino acid sequence로 inverse하겠습니다."],"metadata":{"id":"YNMQkpi7jOt4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-eHn3D1PVIoF"},"outputs":[],"source":["vocab_path = './ESPF/protein_codes_uniprot.txt'\n","bpe_codes_protein = codecs.open(vocab_path)\n","pbpe = BPE(bpe_codes_protein, merges=-1, separator='')\n","sub_csv = pd.read_csv('./ESPF/subword_units_map_uniprot.csv')\n","\n","idx2word_p = sub_csv['index'].values\n","words2idx_p = dict(zip(idx2word_p, range(0, len(idx2word_p))))\n","\n","vocab_path = './ESPF/drug_codes_chembl.txt'\n","bpe_codes_drug = codecs.open(vocab_path)\n","dbpe = BPE(bpe_codes_drug, merges=-1, separator='')\n","sub_csv = pd.read_csv('./ESPF/subword_units_map_chembl.csv')\n","\n","idx2word_d = sub_csv['index'].values\n","words2idx_d = dict(zip(idx2word_d, range(0, len(idx2word_d))))"]},{"cell_type":"markdown","source":["매번 딕셔너리를 호출하여 value로 검색하는 것은 비효율적이므로\n","\n","key, value를 서로 바꾸고 검색하여 index를 모두 SMILES, amino acid sequence로 대치하겠습니다."],"metadata":{"id":"oQrVZWy-jkst"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzHzadhuVIoG"},"outputs":[],"source":["words2idx_p2 = {v:k for k,v in words2idx_p.items()}\n","protein_substructure = [words2idx_p2[key] for key in protein.numpy() if key in words2idx_p2]\n","print(protein_substructure)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDMzVYp-VIoG"},"outputs":[],"source":["words2idx_d2 = {v:k for k,v in words2idx_d.items()}\n","drug_substructure = [words2idx_d2[key] for key in drug.numpy() if key in words2idx_d2]\n","print(drug_substructure)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gaO4Y8gDVIoG"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","interaction_ls = i_v[0][:drug.shape[0], :protein.shape[0]].cpu().detach().numpy()\n","\n","# 10개만 보기\n","plt.figure(figsize=(20, 8))\n","sns.heatmap(interaction_ls[:, :10], annot=True, fmt='.1f', cmap='YlGnBu',\n","            xticklabels=protein_substructure[:10],\n","            yticklabels=drug_substructure)\n","plt.xlabel('Protein Substructure')\n","plt.ylabel('Drug Substructure')\n","plt.title('Interaction Heatmap')\n","plt.show()\n"]},{"cell_type":"markdown","source":["고생하셨습니다. 감사합니다."],"metadata":{"id":"omFUHGMMj9a_"}}],"metadata":{"kernelspec":{"display_name":"hyun","language":"python","name":"hyun"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}